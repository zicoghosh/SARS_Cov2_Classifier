{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Zv-YmR8r7-g2",
    "outputId": "9c69bcd9-cc65-487b-c899-0660ef0a75b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /usr/local/lib/python3.6/dist-packages (1.77)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from biopython) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "7_1fVH1tIJy5",
    "outputId": "c14e3735-f2fd-4258-fee4-ded1668a0bb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (49.1.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoDA2Wt0wN0F"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio.SeqIO import parse\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import math\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, LeakyReLU\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from collections import Counter\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Ud_zFpqmwQO5",
    "outputId": "f0baf0fe-4ad8-44e9-819c-54cb6734714c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-065d4a3b-7086-4c92-b3b0-550f5ad7ff8d\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-065d4a3b-7086-4c92-b3b0-550f5ad7ff8d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final_dates_reduced.csv to final_dates_reduced (1).csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accession</th>\n",
       "      <th>Release_Date</th>\n",
       "      <th>Species</th>\n",
       "      <th>Length</th>\n",
       "      <th>Geo_Location</th>\n",
       "      <th>Host</th>\n",
       "      <th>Isolation_Source</th>\n",
       "      <th>Collection_Date</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MT683386</td>\n",
       "      <td>2020-07-01T00:00:00Z</td>\n",
       "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
       "      <td>29858</td>\n",
       "      <td>USA</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr-May</td>\n",
       "      <td>GGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MT683387</td>\n",
       "      <td>2020-07-01T00:00:00Z</td>\n",
       "      <td>Severe acute respiratory syndrome-related coro...</td>\n",
       "      <td>29854</td>\n",
       "      <td>USA</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr-May</td>\n",
       "      <td>ATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accession  ...                                           Sequence\n",
       "0  MT683386  ...  GGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTT...\n",
       "1  MT683387  ...  ATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGA...\n",
       "\n",
       "[2 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "uploaded = files.upload()\n",
    "df = pd.read_csv(io.StringIO(uploaded['final_dates_reduced.csv'].decode('utf-8')))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "1FDMT3aOLBUT",
    "outputId": "89500f69-07f8-4f26-f91a-d596bfe73d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-metrics\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras-metrics) (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.1.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.18.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.12.0)\n",
      "Installing collected packages: keras-metrics\n",
      "Successfully installed keras-metrics-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8UfrLJUczXK"
   },
   "outputs": [],
   "source": [
    "# df3 = df.sample(frac=1).reset_index(drop=True)\n",
    "# df3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trc5btZmYXim"
   },
   "outputs": [],
   "source": [
    "def string_to_array(my_string):\n",
    "    my_string = my_string.lower()\n",
    "    my_string = re.sub('[^acgt]', 'z', my_string)\n",
    "    my_array = np.array(list(my_string))\n",
    "    return my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o2y2nZXZYb04",
    "outputId": "4b3c8b44-606f-4166-e0b1-2b8ce0fa04b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.array(['a','c','g','t','z']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mysuL1CgYb_X"
   },
   "outputs": [],
   "source": [
    "def ordinal_encoder(my_array):\n",
    "    \n",
    "    integer_encoded = label_encoder.transform(my_array)\n",
    "    float_encoded = integer_encoded.astype(float)\n",
    "    float_encoded[float_encoded == 0] = 0.25 # A\n",
    "    float_encoded[float_encoded == 1] = 0.50 # C\n",
    "    float_encoded[float_encoded == 2] = 0.75 # G\n",
    "    float_encoded[float_encoded == 3] = 1.00 # T\n",
    "    float_encoded[float_encoded == 4] = 0.00 # anything else, z\n",
    "    \n",
    "    return float_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "6SxN0TDAYp7M",
    "outputId": "cb9ddd57-6cfc-4871-819d-4a299c49878f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_seq</th>\n",
       "      <th>Collection_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.75, 0.75, 1.0, 1.0, 1.0, 0.25, 1.0, 0.25, 0...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.25, 1.0, 0.25, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, 0.25, ...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.75, 0.75, 1.0, 0.25, 0.25, 0.5, 0.25, 0.25,...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.75, 0.5, 1.0, 1.0, 0.25, 0.5, 0.75, 0.75, 1...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5592</th>\n",
       "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
       "      <td>Jan-Feb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5593</th>\n",
       "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
       "      <td>Jan-Feb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
       "      <td>Jan-Feb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
       "      <td>Jan-Feb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>[0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...</td>\n",
       "      <td>Jan-Feb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5597 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_seq Collection_Date\n",
       "0     [0.75, 0.75, 1.0, 1.0, 1.0, 0.25, 1.0, 0.25, 0...         Apr-May\n",
       "1     [0.25, 1.0, 0.25, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5...         Apr-May\n",
       "2     [0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, 0.25, ...         Apr-May\n",
       "3     [0.75, 0.75, 1.0, 0.25, 0.25, 0.5, 0.25, 0.25,...         Apr-May\n",
       "4     [0.75, 0.5, 1.0, 1.0, 0.25, 0.5, 0.75, 0.75, 1...         Apr-May\n",
       "...                                                 ...             ...\n",
       "5592  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...         Jan-Feb\n",
       "5593  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...         Jan-Feb\n",
       "5594  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...         Jan-Feb\n",
       "5595  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...         Jan-Feb\n",
       "5596  [0.25, 1.0, 1.0, 0.25, 0.25, 0.25, 0.75, 0.75,...         Jan-Feb\n",
       "\n",
       "[5597 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df[[\"Sequence\",\"Collection_Date\"]]\n",
    "data=data[data[\"Sequence\"].notna()]\n",
    "\n",
    "dummy=[]\n",
    "dum=np.array(dummy)\n",
    "\n",
    "form={\"inp_seq\":dum}\n",
    "\n",
    "seq_df = pd.DataFrame (form, columns = ['inp_seq'])\n",
    "\n",
    "seq_list=[]\n",
    "\n",
    "for idx, seq in enumerate(list(data[\"Sequence\"])):\n",
    "    arr=ordinal_encoder(string_to_array(seq))\n",
    "    seq_list.append(arr)\n",
    "    \n",
    "seq_df[\"inp_seq\"]=seq_list\n",
    "\n",
    "final_data= data.assign(enc_seq=seq_df)\n",
    "\n",
    "final_data=final_data[[\"enc_seq\",\"Collection_Date\"]]\n",
    "\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4XSis1OYya1"
   },
   "outputs": [],
   "source": [
    "def get_maxLen(enc_seq):\n",
    "    \n",
    "    max=0\n",
    "    for row in enc_seq:\n",
    "        #print(type(row))\n",
    "        if(len(row)>max):\n",
    "            max=len(row)\n",
    "    \n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aj-48IzPY1v0"
   },
   "outputs": [],
   "source": [
    "def append_arr(enc_seq, max_len):\n",
    "    \n",
    "    seq_l=list(enc_seq)\n",
    "    for i in range(len(seq_l),max_len):\n",
    "        seq_l.append(0)\n",
    "        \n",
    "    new_seq_ar=np.array(seq_l)\n",
    "\n",
    "        \n",
    "    return new_seq_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "dg9pu_utY3sl",
    "outputId": "eda78e46-58cc-4d81-ed69-5d936327c284"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>padded_enc_seq</th>\n",
       "      <th>Collection_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.75, 0.75, 1.0, 1.0, 1.0, 0.25, 1.0, 0.25, 0...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.25, 1.0, 0.25, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, 0.25, ...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.75, 0.75, 1.0, 0.25, 0.25, 0.5, 0.25, 0.25,...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.75, 0.5, 1.0, 1.0, 0.25, 0.5, 0.75, 0.75, 1...</td>\n",
       "      <td>Apr-May</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      padded_enc_seq Collection_Date\n",
       "0  [0.75, 0.75, 1.0, 1.0, 1.0, 0.25, 1.0, 0.25, 0...         Apr-May\n",
       "1  [0.25, 1.0, 0.25, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5...         Apr-May\n",
       "2  [0.25, 0.25, 0.75, 0.75, 1.0, 1.0, 1.0, 0.25, ...         Apr-May\n",
       "3  [0.75, 0.75, 1.0, 0.25, 0.25, 0.5, 0.25, 0.25,...         Apr-May\n",
       "4  [0.75, 0.5, 1.0, 1.0, 0.25, 0.5, 0.75, 0.75, 1...         Apr-May"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "max_len=get_maxLen(final_data[\"enc_seq\"])\n",
    "# print(\"max_len is\",max_len)\n",
    "# if max_len%2 !=0:\n",
    "#     max_len += 1\n",
    "max_len = 29929\n",
    "\n",
    "n = 1\n",
    "for i in range(1, math.floor(math.sqrt(max_len))):\n",
    "    if max_len%i==0:\n",
    "        n = i\n",
    "\n",
    "padded_seq_list=[]\n",
    "\n",
    "for index, row in final_data.iterrows():\n",
    "    seq_ar=append_arr(row[\"enc_seq\"],max_len)\n",
    "    padded_seq_list.append(seq_ar)\n",
    "\n",
    "dummy=[]\n",
    "dum=np.array(dummy)\n",
    "form={\"padded_enc_seq\":dum}\n",
    "padded_seq_df = pd.DataFrame (form, columns = ['padded_enc_seq'])\n",
    "\n",
    "padded_seq_df[\"padded_enc_seq\"]=padded_seq_list\n",
    "\n",
    "padded_final_data= final_data.assign(padded_enc_seq=padded_seq_df)\n",
    "\n",
    "padded_final_data=padded_final_data[[\"padded_enc_seq\",\"Collection_Date\"]]\n",
    "\n",
    "padded_final_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9c0MjlmY62r"
   },
   "outputs": [],
   "source": [
    "def reshape_seq(seq, m ,n):\n",
    "    \n",
    "    r_seq=np.reshape(seq,(m,n))\n",
    "    \n",
    "    return r_seq\n",
    "\n",
    "re_seqList=[]\n",
    "\n",
    "for index, row in padded_final_data.iterrows():\n",
    "    #seq_ar=reshape_seq(row[\"padded_enc_seq\"],n,max_len//n)\n",
    "    seq_ar=reshape_seq(row[\"padded_enc_seq\"],173, 173)\n",
    "    re_seqList.append(seq_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3HRrJbzQY9q_",
    "outputId": "cca5a509-e6df-45dd-84be-5fc4d1528f46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5597, 173, 173, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(re_seqList)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eKbU-iFYY_pV",
    "outputId": "a72124ee-0ec8-4f73-cf16-e0cc7e6a7758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5597,)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = padded_final_data['Collection_Date'].values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0ueOtHWZChW"
   },
   "outputs": [],
   "source": [
    "loc_classes = list(Y)\n",
    "loc_classes = np.array(loc_classes) \n",
    "loc_classes = list(np.unique(loc_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "OzBtd-yvZF-9",
    "outputId": "680ebc1c-fdf4-446e-e764-6cfc56270396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Apr-May\n",
      "1 Jan-Feb\n",
      "2 Mar\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(loc_classes):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N59F_bhxZIQW",
    "outputId": "87eb24d2-42fa-44bc-ca91-5199230a8b2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5597,)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = padded_final_data['Collection_Date'].apply(loc_classes.index)\n",
    "Y = df2.values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gy2A43YoZLVK"
   },
   "outputs": [],
   "source": [
    "#x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pwtXoj8zOGE"
   },
   "outputs": [],
   "source": [
    "model_best = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21hGNcVup_YI"
   },
   "outputs": [],
   "source": [
    "class CheckMetrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_accs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        X_val, Y_val = self.validation_data[0],self.validation_data[1]\n",
    "                \n",
    "        Y_val = np.argmax(Y_val,axis=1)\n",
    "        \n",
    "        Y_pred = self.model.predict(X_val)\n",
    "        Y_pred = np.argmax(Y_pred,axis=1)\n",
    "\n",
    "        _val_acc = accuracy_score(Y_val, Y_pred)\n",
    "\n",
    "        self.val_accs.append(_val_acc)\n",
    "        \n",
    "        if _val_acc == max(self.val_accs):\n",
    "            print(\"Validation Accuracy has improved. Saving Model.\")\n",
    "            self.model.save('modelCNN.h5')\n",
    "            model_best = self.model\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXdviXB7qA8S"
   },
   "outputs": [],
   "source": [
    "keras_callbacks = CheckMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yh-pWyLobXYQ"
   },
   "source": [
    "K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pBYeZbsIbAZa",
    "outputId": "0097b6af-a51b-4346-c956-5713a5695a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3731 samples, validate on 1866 samples\n",
      "Epoch 1/200\n",
      "3731/3731 [==============================] - 35s 9ms/step - loss: 0.9242 - accuracy: 0.6065 - val_loss: 0.8605 - val_accuracy: 0.5504\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 2/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.7570 - accuracy: 0.6309 - val_loss: 0.9507 - val_accuracy: 0.5504\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 3/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.7141 - accuracy: 0.6457 - val_loss: 0.9648 - val_accuracy: 0.5563\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 4/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6964 - accuracy: 0.6583 - val_loss: 0.7482 - val_accuracy: 0.5729\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 5/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6754 - accuracy: 0.6626 - val_loss: 0.6743 - val_accuracy: 0.6442\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 6/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6639 - accuracy: 0.6717 - val_loss: 0.6398 - val_accuracy: 0.6811\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 7/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6384 - accuracy: 0.6872 - val_loss: 0.7251 - val_accuracy: 0.5868\n",
      "Epoch 8/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6123 - accuracy: 0.7033 - val_loss: 0.6489 - val_accuracy: 0.6763\n",
      "Epoch 9/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5951 - accuracy: 0.7108 - val_loss: 0.6363 - val_accuracy: 0.6731\n",
      "Epoch 10/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5927 - accuracy: 0.7172 - val_loss: 0.6375 - val_accuracy: 0.6961\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 11/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5685 - accuracy: 0.7247 - val_loss: 0.6779 - val_accuracy: 0.6806\n",
      "Epoch 12/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5679 - accuracy: 0.7253 - val_loss: 0.6580 - val_accuracy: 0.6908\n",
      "Epoch 13/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5542 - accuracy: 0.7314 - val_loss: 0.6344 - val_accuracy: 0.6919\n",
      "Epoch 14/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5524 - accuracy: 0.7314 - val_loss: 0.7390 - val_accuracy: 0.6822\n",
      "Epoch 15/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5434 - accuracy: 0.7355 - val_loss: 0.6794 - val_accuracy: 0.6844\n",
      "Epoch 16/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5378 - accuracy: 0.7371 - val_loss: 0.6363 - val_accuracy: 0.7004\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 17/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5398 - accuracy: 0.7440 - val_loss: 0.6523 - val_accuracy: 0.6838\n",
      "Epoch 18/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5244 - accuracy: 0.7534 - val_loss: 0.6694 - val_accuracy: 0.6870\n",
      "Epoch 19/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5269 - accuracy: 0.7446 - val_loss: 0.6950 - val_accuracy: 0.6699\n",
      "Epoch 20/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5223 - accuracy: 0.7483 - val_loss: 0.6513 - val_accuracy: 0.6994\n",
      "Epoch 21/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5177 - accuracy: 0.7521 - val_loss: 0.6245 - val_accuracy: 0.7010\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 22/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5114 - accuracy: 0.7556 - val_loss: 0.6988 - val_accuracy: 0.6892\n",
      "Epoch 23/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5039 - accuracy: 0.7596 - val_loss: 0.6826 - val_accuracy: 0.6897\n",
      "Epoch 24/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5124 - accuracy: 0.7604 - val_loss: 0.6421 - val_accuracy: 0.6940\n",
      "Epoch 25/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5059 - accuracy: 0.7534 - val_loss: 0.6674 - val_accuracy: 0.6902\n",
      "Epoch 26/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4918 - accuracy: 0.7590 - val_loss: 0.6392 - val_accuracy: 0.7010\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 27/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5049 - accuracy: 0.7604 - val_loss: 0.6531 - val_accuracy: 0.6881\n",
      "Epoch 28/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4903 - accuracy: 0.7668 - val_loss: 0.7324 - val_accuracy: 0.7026\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 29/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4838 - accuracy: 0.7609 - val_loss: 0.7092 - val_accuracy: 0.6881\n",
      "Epoch 30/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4860 - accuracy: 0.7580 - val_loss: 0.6992 - val_accuracy: 0.7015\n",
      "Epoch 31/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4875 - accuracy: 0.7625 - val_loss: 0.7112 - val_accuracy: 0.6929\n",
      "Epoch 32/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4789 - accuracy: 0.7692 - val_loss: 0.6563 - val_accuracy: 0.7010\n",
      "Epoch 33/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4717 - accuracy: 0.7741 - val_loss: 0.6797 - val_accuracy: 0.6913\n",
      "Epoch 34/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4798 - accuracy: 0.7700 - val_loss: 0.6718 - val_accuracy: 0.6961\n",
      "Epoch 35/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4718 - accuracy: 0.7797 - val_loss: 0.6933 - val_accuracy: 0.7079\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 36/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4695 - accuracy: 0.7773 - val_loss: 0.6793 - val_accuracy: 0.7074\n",
      "Epoch 37/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4612 - accuracy: 0.7810 - val_loss: 0.6719 - val_accuracy: 0.6849\n",
      "Epoch 38/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4619 - accuracy: 0.7888 - val_loss: 0.6963 - val_accuracy: 0.6999\n",
      "Epoch 39/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4648 - accuracy: 0.7759 - val_loss: 0.6844 - val_accuracy: 0.6929\n",
      "Epoch 40/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4596 - accuracy: 0.7816 - val_loss: 0.7003 - val_accuracy: 0.6977\n",
      "Epoch 41/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4572 - accuracy: 0.7856 - val_loss: 0.6651 - val_accuracy: 0.6892\n",
      "Epoch 42/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4607 - accuracy: 0.7791 - val_loss: 0.7759 - val_accuracy: 0.6902\n",
      "Epoch 43/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4477 - accuracy: 0.7909 - val_loss: 0.7389 - val_accuracy: 0.6913\n",
      "Epoch 44/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4513 - accuracy: 0.7829 - val_loss: 0.6884 - val_accuracy: 0.6983\n",
      "Epoch 45/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4521 - accuracy: 0.7853 - val_loss: 0.7122 - val_accuracy: 0.7069\n",
      "Epoch 46/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4509 - accuracy: 0.7850 - val_loss: 0.7129 - val_accuracy: 0.7036\n",
      "Epoch 47/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4478 - accuracy: 0.7931 - val_loss: 0.7043 - val_accuracy: 0.6935\n",
      "Epoch 48/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4330 - accuracy: 0.8006 - val_loss: 0.7476 - val_accuracy: 0.6967\n",
      "Epoch 49/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4431 - accuracy: 0.7909 - val_loss: 0.7384 - val_accuracy: 0.6844\n",
      "Epoch 50/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4477 - accuracy: 0.7885 - val_loss: 0.6782 - val_accuracy: 0.6994\n",
      "Epoch 51/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4408 - accuracy: 0.7984 - val_loss: 0.6966 - val_accuracy: 0.7160\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 52/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4334 - accuracy: 0.7877 - val_loss: 0.6762 - val_accuracy: 0.6977\n",
      "Epoch 53/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4441 - accuracy: 0.7875 - val_loss: 0.7347 - val_accuracy: 0.6999\n",
      "Epoch 54/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4328 - accuracy: 0.7968 - val_loss: 0.7725 - val_accuracy: 0.6956\n",
      "Epoch 55/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4344 - accuracy: 0.7955 - val_loss: 0.6701 - val_accuracy: 0.6999\n",
      "Epoch 56/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4246 - accuracy: 0.8041 - val_loss: 0.7507 - val_accuracy: 0.7010\n",
      "Epoch 57/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4270 - accuracy: 0.7976 - val_loss: 0.6908 - val_accuracy: 0.6961\n",
      "Epoch 58/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4495 - accuracy: 0.7920 - val_loss: 0.7348 - val_accuracy: 0.6977\n",
      "Epoch 59/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4232 - accuracy: 0.8014 - val_loss: 0.7780 - val_accuracy: 0.6919\n",
      "Epoch 60/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4305 - accuracy: 0.8006 - val_loss: 0.7750 - val_accuracy: 0.6994\n",
      "Epoch 61/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4231 - accuracy: 0.8100 - val_loss: 0.8633 - val_accuracy: 0.6693\n",
      "Epoch 62/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4201 - accuracy: 0.8094 - val_loss: 0.7402 - val_accuracy: 0.6935\n",
      "Epoch 63/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4169 - accuracy: 0.8062 - val_loss: 0.7285 - val_accuracy: 0.6994\n",
      "Epoch 64/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4158 - accuracy: 0.8035 - val_loss: 0.7176 - val_accuracy: 0.6983\n",
      "Epoch 65/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4162 - accuracy: 0.8073 - val_loss: 0.8093 - val_accuracy: 0.7053\n",
      "Epoch 66/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4134 - accuracy: 0.7998 - val_loss: 0.7174 - val_accuracy: 0.6897\n",
      "Epoch 67/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4099 - accuracy: 0.8017 - val_loss: 0.7241 - val_accuracy: 0.7069\n",
      "Epoch 68/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4134 - accuracy: 0.8070 - val_loss: 0.8269 - val_accuracy: 0.7026\n",
      "Epoch 69/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4160 - accuracy: 0.8051 - val_loss: 0.7830 - val_accuracy: 0.7069\n",
      "Epoch 70/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4076 - accuracy: 0.8046 - val_loss: 0.7833 - val_accuracy: 0.7122\n",
      "Epoch 71/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4077 - accuracy: 0.8121 - val_loss: 0.7123 - val_accuracy: 0.7165\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 72/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4105 - accuracy: 0.8118 - val_loss: 0.8056 - val_accuracy: 0.6983\n",
      "Epoch 73/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4004 - accuracy: 0.8097 - val_loss: 0.8457 - val_accuracy: 0.7026\n",
      "Epoch 74/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4037 - accuracy: 0.8164 - val_loss: 0.7641 - val_accuracy: 0.7047\n",
      "Epoch 75/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3963 - accuracy: 0.8204 - val_loss: 0.7685 - val_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4007 - accuracy: 0.8145 - val_loss: 0.7209 - val_accuracy: 0.6977\n",
      "Epoch 77/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3961 - accuracy: 0.8212 - val_loss: 0.7632 - val_accuracy: 0.6908\n",
      "Epoch 78/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3884 - accuracy: 0.8236 - val_loss: 0.7863 - val_accuracy: 0.7240\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 79/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3912 - accuracy: 0.8223 - val_loss: 0.8233 - val_accuracy: 0.7181\n",
      "Epoch 80/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3989 - accuracy: 0.8228 - val_loss: 0.8759 - val_accuracy: 0.7063\n",
      "Epoch 81/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3909 - accuracy: 0.8215 - val_loss: 0.8600 - val_accuracy: 0.6844\n",
      "Epoch 82/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3804 - accuracy: 0.8282 - val_loss: 0.7240 - val_accuracy: 0.7122\n",
      "Epoch 83/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3846 - accuracy: 0.8255 - val_loss: 0.8001 - val_accuracy: 0.7192\n",
      "Epoch 84/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3765 - accuracy: 0.8244 - val_loss: 0.7492 - val_accuracy: 0.7138\n",
      "Epoch 85/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3794 - accuracy: 0.8311 - val_loss: 0.8081 - val_accuracy: 0.7299\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 86/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3865 - accuracy: 0.8258 - val_loss: 0.6748 - val_accuracy: 0.7245\n",
      "Epoch 87/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3923 - accuracy: 0.8236 - val_loss: 0.8661 - val_accuracy: 0.7160\n",
      "Epoch 88/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3791 - accuracy: 0.8346 - val_loss: 0.7858 - val_accuracy: 0.7272\n",
      "Epoch 89/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3832 - accuracy: 0.8370 - val_loss: 0.8346 - val_accuracy: 0.7272\n",
      "Epoch 90/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3884 - accuracy: 0.8287 - val_loss: 0.7408 - val_accuracy: 0.7133\n",
      "Epoch 91/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3721 - accuracy: 0.8319 - val_loss: 0.7913 - val_accuracy: 0.7111\n",
      "Epoch 92/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3756 - accuracy: 0.8400 - val_loss: 0.8863 - val_accuracy: 0.7010\n",
      "Epoch 93/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3663 - accuracy: 0.8354 - val_loss: 0.8086 - val_accuracy: 0.7181\n",
      "Epoch 94/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3758 - accuracy: 0.8325 - val_loss: 0.7511 - val_accuracy: 0.7245\n",
      "Epoch 95/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3733 - accuracy: 0.8309 - val_loss: 0.8159 - val_accuracy: 0.7347\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 96/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3671 - accuracy: 0.8386 - val_loss: 0.7485 - val_accuracy: 0.7224\n",
      "Epoch 97/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3666 - accuracy: 0.8357 - val_loss: 0.9297 - val_accuracy: 0.7192\n",
      "Epoch 98/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3770 - accuracy: 0.8325 - val_loss: 0.8387 - val_accuracy: 0.7160\n",
      "Epoch 99/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3613 - accuracy: 0.8443 - val_loss: 0.7355 - val_accuracy: 0.7192\n",
      "Epoch 100/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3633 - accuracy: 0.8341 - val_loss: 0.8038 - val_accuracy: 0.7267\n",
      "Epoch 101/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3633 - accuracy: 0.8362 - val_loss: 0.7731 - val_accuracy: 0.7111\n",
      "Epoch 102/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3632 - accuracy: 0.8346 - val_loss: 0.7763 - val_accuracy: 0.7122\n",
      "Epoch 103/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3527 - accuracy: 0.8421 - val_loss: 0.7334 - val_accuracy: 0.7245\n",
      "Epoch 104/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3589 - accuracy: 0.8392 - val_loss: 0.8560 - val_accuracy: 0.7245\n",
      "Epoch 105/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3561 - accuracy: 0.8424 - val_loss: 0.9552 - val_accuracy: 0.7288\n",
      "Epoch 106/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3628 - accuracy: 0.8427 - val_loss: 0.8331 - val_accuracy: 0.7315\n",
      "Epoch 107/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3556 - accuracy: 0.8427 - val_loss: 0.8239 - val_accuracy: 0.7267\n",
      "Epoch 108/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3554 - accuracy: 0.8494 - val_loss: 0.8358 - val_accuracy: 0.7310\n",
      "Epoch 109/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3499 - accuracy: 0.8427 - val_loss: 0.9314 - val_accuracy: 0.7342\n",
      "Epoch 110/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3585 - accuracy: 0.8427 - val_loss: 0.7813 - val_accuracy: 0.7304\n",
      "Epoch 111/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3538 - accuracy: 0.8421 - val_loss: 0.8412 - val_accuracy: 0.7251\n",
      "Epoch 112/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3446 - accuracy: 0.8453 - val_loss: 0.9067 - val_accuracy: 0.7347\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 113/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3418 - accuracy: 0.8507 - val_loss: 0.9168 - val_accuracy: 0.7128\n",
      "Epoch 114/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3459 - accuracy: 0.8464 - val_loss: 0.9667 - val_accuracy: 0.7331\n",
      "Epoch 115/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3471 - accuracy: 0.8373 - val_loss: 0.9544 - val_accuracy: 0.7337\n",
      "Epoch 116/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3403 - accuracy: 0.8507 - val_loss: 0.8378 - val_accuracy: 0.7101\n",
      "Epoch 117/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3576 - accuracy: 0.8480 - val_loss: 0.7882 - val_accuracy: 0.7331\n",
      "Epoch 118/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3387 - accuracy: 0.8467 - val_loss: 0.8896 - val_accuracy: 0.7219\n",
      "Epoch 119/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3446 - accuracy: 0.8462 - val_loss: 0.8151 - val_accuracy: 0.7208\n",
      "Epoch 120/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3348 - accuracy: 0.8518 - val_loss: 0.8695 - val_accuracy: 0.6983\n",
      "Epoch 121/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3347 - accuracy: 0.8507 - val_loss: 0.9021 - val_accuracy: 0.7310\n",
      "Epoch 122/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3391 - accuracy: 0.8472 - val_loss: 0.8372 - val_accuracy: 0.7288\n",
      "Epoch 123/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3336 - accuracy: 0.8512 - val_loss: 0.8678 - val_accuracy: 0.7326\n",
      "Epoch 124/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3461 - accuracy: 0.8451 - val_loss: 0.8532 - val_accuracy: 0.7299\n",
      "Epoch 125/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3496 - accuracy: 0.8521 - val_loss: 0.8544 - val_accuracy: 0.7219\n",
      "Epoch 126/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3379 - accuracy: 0.8515 - val_loss: 0.8684 - val_accuracy: 0.7363\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 127/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3323 - accuracy: 0.8539 - val_loss: 0.9089 - val_accuracy: 0.7262\n",
      "Epoch 128/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3275 - accuracy: 0.8539 - val_loss: 0.8407 - val_accuracy: 0.7283\n",
      "Epoch 129/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3250 - accuracy: 0.8569 - val_loss: 0.7705 - val_accuracy: 0.7283\n",
      "Epoch 130/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3239 - accuracy: 0.8628 - val_loss: 0.9942 - val_accuracy: 0.7229\n",
      "Epoch 131/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3342 - accuracy: 0.8523 - val_loss: 0.9130 - val_accuracy: 0.6838\n",
      "Epoch 132/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3375 - accuracy: 0.8526 - val_loss: 0.9033 - val_accuracy: 0.7229\n",
      "Epoch 133/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3343 - accuracy: 0.8443 - val_loss: 0.7848 - val_accuracy: 0.7213\n",
      "Epoch 134/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3303 - accuracy: 0.8561 - val_loss: 0.8962 - val_accuracy: 0.7117\n",
      "Epoch 135/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3190 - accuracy: 0.8571 - val_loss: 0.8606 - val_accuracy: 0.7229\n",
      "Epoch 136/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3146 - accuracy: 0.8606 - val_loss: 0.8676 - val_accuracy: 0.7256\n",
      "Epoch 137/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3336 - accuracy: 0.8547 - val_loss: 0.8347 - val_accuracy: 0.7197\n",
      "Epoch 138/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3171 - accuracy: 0.8649 - val_loss: 1.0550 - val_accuracy: 0.7278\n",
      "Epoch 139/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3245 - accuracy: 0.8569 - val_loss: 0.9024 - val_accuracy: 0.7144\n",
      "Epoch 140/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3284 - accuracy: 0.8596 - val_loss: 0.9098 - val_accuracy: 0.7160\n",
      "Epoch 141/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3282 - accuracy: 0.8547 - val_loss: 0.8818 - val_accuracy: 0.7229\n",
      "Epoch 142/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3214 - accuracy: 0.8574 - val_loss: 0.8352 - val_accuracy: 0.7353\n",
      "Epoch 143/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3243 - accuracy: 0.8598 - val_loss: 0.9016 - val_accuracy: 0.7342\n",
      "Epoch 144/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3136 - accuracy: 0.8598 - val_loss: 0.9571 - val_accuracy: 0.7063\n",
      "Epoch 145/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3149 - accuracy: 0.8671 - val_loss: 0.7498 - val_accuracy: 0.7245\n",
      "Epoch 146/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3106 - accuracy: 0.8668 - val_loss: 1.0609 - val_accuracy: 0.6940\n",
      "Epoch 147/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3234 - accuracy: 0.8596 - val_loss: 0.9108 - val_accuracy: 0.7299\n",
      "Epoch 148/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3190 - accuracy: 0.8553 - val_loss: 1.0860 - val_accuracy: 0.7283\n",
      "Epoch 149/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3149 - accuracy: 0.8622 - val_loss: 0.8860 - val_accuracy: 0.7170\n",
      "Epoch 150/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3179 - accuracy: 0.8609 - val_loss: 0.8404 - val_accuracy: 0.7288\n",
      "Epoch 151/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3011 - accuracy: 0.8692 - val_loss: 0.8826 - val_accuracy: 0.7186\n",
      "Epoch 152/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3115 - accuracy: 0.8628 - val_loss: 0.9740 - val_accuracy: 0.7342\n",
      "Epoch 153/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3101 - accuracy: 0.8660 - val_loss: 0.9580 - val_accuracy: 0.7149\n",
      "Epoch 154/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3051 - accuracy: 0.8679 - val_loss: 0.8967 - val_accuracy: 0.7154\n",
      "Epoch 155/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3138 - accuracy: 0.8630 - val_loss: 1.0209 - val_accuracy: 0.7395\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 156/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3030 - accuracy: 0.8697 - val_loss: 0.8466 - val_accuracy: 0.7395\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 157/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3080 - accuracy: 0.8657 - val_loss: 0.9632 - val_accuracy: 0.7363\n",
      "Epoch 158/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3020 - accuracy: 0.8700 - val_loss: 0.9238 - val_accuracy: 0.7310\n",
      "Epoch 159/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2985 - accuracy: 0.8689 - val_loss: 0.7834 - val_accuracy: 0.7363\n",
      "Epoch 160/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2902 - accuracy: 0.8772 - val_loss: 0.9557 - val_accuracy: 0.7278\n",
      "Epoch 161/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3031 - accuracy: 0.8703 - val_loss: 1.0443 - val_accuracy: 0.7229\n",
      "Epoch 162/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3019 - accuracy: 0.8748 - val_loss: 0.9838 - val_accuracy: 0.7326\n",
      "Epoch 163/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3038 - accuracy: 0.8671 - val_loss: 0.9153 - val_accuracy: 0.7353\n",
      "Epoch 164/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2996 - accuracy: 0.8646 - val_loss: 0.9982 - val_accuracy: 0.7262\n",
      "Epoch 165/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3035 - accuracy: 0.8700 - val_loss: 0.8769 - val_accuracy: 0.7363\n",
      "Epoch 166/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2902 - accuracy: 0.8740 - val_loss: 1.0500 - val_accuracy: 0.7219\n",
      "Epoch 167/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2849 - accuracy: 0.8772 - val_loss: 0.9537 - val_accuracy: 0.7320\n",
      "Epoch 168/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2835 - accuracy: 0.8767 - val_loss: 0.9960 - val_accuracy: 0.7412\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 169/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2967 - accuracy: 0.8762 - val_loss: 0.8804 - val_accuracy: 0.7363\n",
      "Epoch 170/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2873 - accuracy: 0.8759 - val_loss: 1.2097 - val_accuracy: 0.7283\n",
      "Epoch 171/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2953 - accuracy: 0.8697 - val_loss: 1.0403 - val_accuracy: 0.7304\n",
      "Epoch 172/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2904 - accuracy: 0.8708 - val_loss: 1.0032 - val_accuracy: 0.7337\n",
      "Epoch 173/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2914 - accuracy: 0.8767 - val_loss: 0.9592 - val_accuracy: 0.7267\n",
      "Epoch 174/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2806 - accuracy: 0.8759 - val_loss: 0.8460 - val_accuracy: 0.7262\n",
      "Epoch 175/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2841 - accuracy: 0.8727 - val_loss: 0.9264 - val_accuracy: 0.7395\n",
      "Epoch 176/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2793 - accuracy: 0.8794 - val_loss: 1.0790 - val_accuracy: 0.7181\n",
      "Epoch 177/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2843 - accuracy: 0.8764 - val_loss: 0.9991 - val_accuracy: 0.7299\n",
      "Epoch 178/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2848 - accuracy: 0.8732 - val_loss: 1.1321 - val_accuracy: 0.7385\n",
      "Epoch 179/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2860 - accuracy: 0.8756 - val_loss: 0.9536 - val_accuracy: 0.7144\n",
      "Epoch 180/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2848 - accuracy: 0.8791 - val_loss: 0.9837 - val_accuracy: 0.7272\n",
      "Epoch 181/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2718 - accuracy: 0.8751 - val_loss: 1.0933 - val_accuracy: 0.7417\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 182/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2793 - accuracy: 0.8772 - val_loss: 1.0616 - val_accuracy: 0.7358\n",
      "Epoch 183/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2884 - accuracy: 0.8751 - val_loss: 1.0218 - val_accuracy: 0.7299\n",
      "Epoch 184/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2719 - accuracy: 0.8869 - val_loss: 0.8345 - val_accuracy: 0.7347\n",
      "Epoch 185/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2693 - accuracy: 0.8837 - val_loss: 0.9523 - val_accuracy: 0.7256\n",
      "Epoch 186/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2810 - accuracy: 0.8810 - val_loss: 0.9572 - val_accuracy: 0.7192\n",
      "Epoch 187/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2817 - accuracy: 0.8730 - val_loss: 1.0035 - val_accuracy: 0.7235\n",
      "Epoch 188/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2854 - accuracy: 0.8810 - val_loss: 1.0962 - val_accuracy: 0.7192\n",
      "Epoch 189/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2807 - accuracy: 0.8821 - val_loss: 1.0357 - val_accuracy: 0.7256\n",
      "Epoch 190/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2611 - accuracy: 0.8831 - val_loss: 0.9642 - val_accuracy: 0.7267\n",
      "Epoch 191/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2750 - accuracy: 0.8839 - val_loss: 0.9975 - val_accuracy: 0.7337\n",
      "Epoch 192/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2660 - accuracy: 0.8850 - val_loss: 1.0840 - val_accuracy: 0.7379\n",
      "Epoch 193/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2704 - accuracy: 0.8834 - val_loss: 0.9605 - val_accuracy: 0.7417\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 194/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2618 - accuracy: 0.8866 - val_loss: 1.0239 - val_accuracy: 0.7353\n",
      "Epoch 195/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2621 - accuracy: 0.8882 - val_loss: 1.2372 - val_accuracy: 0.7256\n",
      "Epoch 196/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2737 - accuracy: 0.8799 - val_loss: 1.1120 - val_accuracy: 0.7138\n",
      "Epoch 197/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2638 - accuracy: 0.8775 - val_loss: 0.8766 - val_accuracy: 0.7358\n",
      "Epoch 198/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2575 - accuracy: 0.8805 - val_loss: 1.1643 - val_accuracy: 0.7385\n",
      "Epoch 199/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2705 - accuracy: 0.8904 - val_loss: 0.9860 - val_accuracy: 0.7278\n",
      "Epoch 200/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2582 - accuracy: 0.8839 - val_loss: 0.9413 - val_accuracy: 0.7401\n",
      "accuracy: 74.17%\n",
      "Confusion Matrix - Test\n",
      "[[540   2 257]\n",
      " [  1  21  18]\n",
      " [197   7 823]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Apr-May       0.73      0.68      0.70       799\n",
      "     Jan-Feb       0.70      0.53      0.60        40\n",
      "         Mar       0.75      0.80      0.77      1027\n",
      "\n",
      "    accuracy                           0.74      1866\n",
      "   macro avg       0.73      0.67      0.69      1866\n",
      "weighted avg       0.74      0.74      0.74      1866\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Train on 3731 samples, validate on 1866 samples\n",
      "Epoch 1/200\n",
      "3731/3731 [==============================] - 27s 7ms/step - loss: 0.9587 - accuracy: 0.5897 - val_loss: 0.8451 - val_accuracy: 0.5504\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 2/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.7735 - accuracy: 0.6189 - val_loss: 0.8143 - val_accuracy: 0.5504\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 3/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.7079 - accuracy: 0.6478 - val_loss: 0.8644 - val_accuracy: 0.5536\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 4/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.7059 - accuracy: 0.6438 - val_loss: 0.7560 - val_accuracy: 0.5616\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 5/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6652 - accuracy: 0.6655 - val_loss: 0.6746 - val_accuracy: 0.6785\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 6/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6487 - accuracy: 0.6733 - val_loss: 0.6744 - val_accuracy: 0.6688\n",
      "Epoch 7/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6304 - accuracy: 0.6800 - val_loss: 0.6494 - val_accuracy: 0.6833\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 8/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.6114 - accuracy: 0.6966 - val_loss: 0.6753 - val_accuracy: 0.6527\n",
      "Epoch 9/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5947 - accuracy: 0.7001 - val_loss: 0.7082 - val_accuracy: 0.6683\n",
      "Epoch 10/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5967 - accuracy: 0.7049 - val_loss: 0.6335 - val_accuracy: 0.6854\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 11/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5813 - accuracy: 0.7025 - val_loss: 0.6903 - val_accuracy: 0.6420\n",
      "Epoch 12/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5781 - accuracy: 0.7132 - val_loss: 0.6120 - val_accuracy: 0.7133\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 13/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5606 - accuracy: 0.7253 - val_loss: 0.9508 - val_accuracy: 0.5043\n",
      "Epoch 14/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5582 - accuracy: 0.7282 - val_loss: 0.6450 - val_accuracy: 0.6699\n",
      "Epoch 15/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5568 - accuracy: 0.7285 - val_loss: 0.6440 - val_accuracy: 0.6940\n",
      "Epoch 16/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5406 - accuracy: 0.7381 - val_loss: 0.6229 - val_accuracy: 0.7058\n",
      "Epoch 17/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5387 - accuracy: 0.7406 - val_loss: 0.6283 - val_accuracy: 0.7047\n",
      "Epoch 18/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5402 - accuracy: 0.7389 - val_loss: 0.8439 - val_accuracy: 0.6613\n",
      "Epoch 19/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5264 - accuracy: 0.7515 - val_loss: 0.6358 - val_accuracy: 0.7111\n",
      "Epoch 20/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5343 - accuracy: 0.7430 - val_loss: 0.6820 - val_accuracy: 0.6581\n",
      "Epoch 21/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5192 - accuracy: 0.7470 - val_loss: 0.6381 - val_accuracy: 0.7224\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 22/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5173 - accuracy: 0.7483 - val_loss: 0.6269 - val_accuracy: 0.7020\n",
      "Epoch 23/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5112 - accuracy: 0.7510 - val_loss: 0.6375 - val_accuracy: 0.7090\n",
      "Epoch 24/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4988 - accuracy: 0.7507 - val_loss: 0.6503 - val_accuracy: 0.6817\n",
      "Epoch 25/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.5032 - accuracy: 0.7684 - val_loss: 0.6002 - val_accuracy: 0.7079\n",
      "Epoch 26/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4971 - accuracy: 0.7636 - val_loss: 0.7373 - val_accuracy: 0.7079\n",
      "Epoch 27/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4981 - accuracy: 0.7596 - val_loss: 0.6455 - val_accuracy: 0.7176\n",
      "Epoch 28/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4913 - accuracy: 0.7647 - val_loss: 0.6634 - val_accuracy: 0.6994\n",
      "Epoch 29/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4789 - accuracy: 0.7674 - val_loss: 0.6725 - val_accuracy: 0.7278\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 30/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4947 - accuracy: 0.7730 - val_loss: 0.6928 - val_accuracy: 0.7095\n",
      "Epoch 31/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4877 - accuracy: 0.7690 - val_loss: 0.7250 - val_accuracy: 0.6699\n",
      "Epoch 32/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4782 - accuracy: 0.7687 - val_loss: 0.6330 - val_accuracy: 0.7111\n",
      "Epoch 33/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4723 - accuracy: 0.7711 - val_loss: 0.8199 - val_accuracy: 0.6827\n",
      "Epoch 34/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4740 - accuracy: 0.7708 - val_loss: 0.6573 - val_accuracy: 0.7058\n",
      "Epoch 35/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4684 - accuracy: 0.7810 - val_loss: 0.6921 - val_accuracy: 0.7058\n",
      "Epoch 36/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4570 - accuracy: 0.7899 - val_loss: 0.6693 - val_accuracy: 0.7111\n",
      "Epoch 37/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4623 - accuracy: 0.7797 - val_loss: 0.6515 - val_accuracy: 0.7053\n",
      "Epoch 38/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4619 - accuracy: 0.7783 - val_loss: 0.7059 - val_accuracy: 0.6833\n",
      "Epoch 39/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4636 - accuracy: 0.7824 - val_loss: 0.6942 - val_accuracy: 0.7085\n",
      "Epoch 40/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4563 - accuracy: 0.7743 - val_loss: 0.7374 - val_accuracy: 0.7128\n",
      "Epoch 41/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4534 - accuracy: 0.7824 - val_loss: 0.6707 - val_accuracy: 0.7235\n",
      "Epoch 42/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4511 - accuracy: 0.7786 - val_loss: 0.6575 - val_accuracy: 0.7160\n",
      "Epoch 43/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4391 - accuracy: 0.7917 - val_loss: 0.6432 - val_accuracy: 0.7208\n",
      "Epoch 44/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4454 - accuracy: 0.7858 - val_loss: 0.6669 - val_accuracy: 0.7342\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 45/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4422 - accuracy: 0.7915 - val_loss: 0.6555 - val_accuracy: 0.7133\n",
      "Epoch 46/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4306 - accuracy: 0.7982 - val_loss: 0.7229 - val_accuracy: 0.7160\n",
      "Epoch 47/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4336 - accuracy: 0.7968 - val_loss: 0.7000 - val_accuracy: 0.7004\n",
      "Epoch 48/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4355 - accuracy: 0.7936 - val_loss: 0.8385 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4315 - accuracy: 0.7974 - val_loss: 0.6505 - val_accuracy: 0.7224\n",
      "Epoch 50/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4284 - accuracy: 0.8038 - val_loss: 0.6753 - val_accuracy: 0.7122\n",
      "Epoch 51/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4271 - accuracy: 0.7987 - val_loss: 0.7286 - val_accuracy: 0.7154\n",
      "Epoch 52/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4258 - accuracy: 0.8006 - val_loss: 0.6399 - val_accuracy: 0.7235\n",
      "Epoch 53/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4249 - accuracy: 0.7995 - val_loss: 0.6807 - val_accuracy: 0.7235\n",
      "Epoch 54/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4261 - accuracy: 0.7950 - val_loss: 0.7364 - val_accuracy: 0.7181\n",
      "Epoch 55/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4227 - accuracy: 0.7976 - val_loss: 0.7512 - val_accuracy: 0.7079\n",
      "Epoch 56/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4219 - accuracy: 0.8014 - val_loss: 0.6732 - val_accuracy: 0.7079\n",
      "Epoch 57/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4211 - accuracy: 0.8062 - val_loss: 0.6811 - val_accuracy: 0.7203\n",
      "Epoch 58/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4179 - accuracy: 0.8051 - val_loss: 0.6726 - val_accuracy: 0.7181\n",
      "Epoch 59/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4116 - accuracy: 0.8065 - val_loss: 0.6937 - val_accuracy: 0.7186\n",
      "Epoch 60/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4129 - accuracy: 0.8054 - val_loss: 0.7160 - val_accuracy: 0.7058\n",
      "Epoch 61/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4109 - accuracy: 0.8084 - val_loss: 0.8083 - val_accuracy: 0.7203\n",
      "Epoch 62/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4027 - accuracy: 0.8110 - val_loss: 0.7327 - val_accuracy: 0.7128\n",
      "Epoch 63/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4088 - accuracy: 0.8161 - val_loss: 0.7855 - val_accuracy: 0.7074\n",
      "Epoch 64/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4109 - accuracy: 0.8118 - val_loss: 0.7026 - val_accuracy: 0.7069\n",
      "Epoch 65/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4074 - accuracy: 0.8129 - val_loss: 0.8204 - val_accuracy: 0.6940\n",
      "Epoch 66/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4121 - accuracy: 0.8124 - val_loss: 0.7534 - val_accuracy: 0.7133\n",
      "Epoch 67/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.4076 - accuracy: 0.8089 - val_loss: 0.7138 - val_accuracy: 0.7063\n",
      "Epoch 68/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3985 - accuracy: 0.8175 - val_loss: 0.7816 - val_accuracy: 0.6790\n",
      "Epoch 69/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3986 - accuracy: 0.8183 - val_loss: 0.8212 - val_accuracy: 0.6988\n",
      "Epoch 70/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3999 - accuracy: 0.8169 - val_loss: 0.7816 - val_accuracy: 0.7128\n",
      "Epoch 71/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3939 - accuracy: 0.8137 - val_loss: 0.7600 - val_accuracy: 0.7010\n",
      "Epoch 72/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3925 - accuracy: 0.8113 - val_loss: 0.8049 - val_accuracy: 0.7170\n",
      "Epoch 73/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3994 - accuracy: 0.8127 - val_loss: 0.9743 - val_accuracy: 0.7128\n",
      "Epoch 74/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3851 - accuracy: 0.8207 - val_loss: 0.9699 - val_accuracy: 0.7186\n",
      "Epoch 75/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3943 - accuracy: 0.8135 - val_loss: 0.8114 - val_accuracy: 0.7004\n",
      "Epoch 76/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3865 - accuracy: 0.8204 - val_loss: 0.8114 - val_accuracy: 0.7181\n",
      "Epoch 77/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3881 - accuracy: 0.8188 - val_loss: 0.8656 - val_accuracy: 0.7138\n",
      "Epoch 78/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3838 - accuracy: 0.8234 - val_loss: 0.7814 - val_accuracy: 0.7036\n",
      "Epoch 79/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3839 - accuracy: 0.8223 - val_loss: 0.8362 - val_accuracy: 0.6929\n",
      "Epoch 80/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3930 - accuracy: 0.8207 - val_loss: 0.7736 - val_accuracy: 0.6908\n",
      "Epoch 81/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3805 - accuracy: 0.8231 - val_loss: 0.8820 - val_accuracy: 0.7288\n",
      "Epoch 82/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3785 - accuracy: 0.8212 - val_loss: 0.8043 - val_accuracy: 0.7133\n",
      "Epoch 83/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3836 - accuracy: 0.8167 - val_loss: 0.8178 - val_accuracy: 0.7020\n",
      "Epoch 84/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3748 - accuracy: 0.8244 - val_loss: 0.8057 - val_accuracy: 0.7128\n",
      "Epoch 85/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3737 - accuracy: 0.8285 - val_loss: 0.8444 - val_accuracy: 0.7117\n",
      "Epoch 86/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3826 - accuracy: 0.8242 - val_loss: 0.7711 - val_accuracy: 0.7063\n",
      "Epoch 87/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3820 - accuracy: 0.8247 - val_loss: 1.0094 - val_accuracy: 0.6870\n",
      "Epoch 88/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3813 - accuracy: 0.8239 - val_loss: 0.8147 - val_accuracy: 0.7165\n",
      "Epoch 89/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3756 - accuracy: 0.8231 - val_loss: 0.7584 - val_accuracy: 0.6951\n",
      "Epoch 90/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3645 - accuracy: 0.8309 - val_loss: 0.8939 - val_accuracy: 0.7053\n",
      "Epoch 91/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3688 - accuracy: 0.8269 - val_loss: 0.7791 - val_accuracy: 0.7128\n",
      "Epoch 92/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3768 - accuracy: 0.8236 - val_loss: 1.1252 - val_accuracy: 0.6961\n",
      "Epoch 93/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3623 - accuracy: 0.8362 - val_loss: 0.8763 - val_accuracy: 0.7101\n",
      "Epoch 94/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3574 - accuracy: 0.8352 - val_loss: 0.9546 - val_accuracy: 0.7197\n",
      "Epoch 95/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3615 - accuracy: 0.8354 - val_loss: 0.9095 - val_accuracy: 0.7128\n",
      "Epoch 96/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3678 - accuracy: 0.8306 - val_loss: 0.9060 - val_accuracy: 0.7165\n",
      "Epoch 97/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3684 - accuracy: 0.8325 - val_loss: 1.0061 - val_accuracy: 0.7181\n",
      "Epoch 98/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3613 - accuracy: 0.8341 - val_loss: 0.7599 - val_accuracy: 0.7165\n",
      "Epoch 99/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3584 - accuracy: 0.8338 - val_loss: 0.8690 - val_accuracy: 0.7144\n",
      "Epoch 100/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3499 - accuracy: 0.8411 - val_loss: 0.8398 - val_accuracy: 0.7224\n",
      "Epoch 101/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3487 - accuracy: 0.8400 - val_loss: 0.7981 - val_accuracy: 0.7208\n",
      "Epoch 102/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3475 - accuracy: 0.8486 - val_loss: 0.8814 - val_accuracy: 0.7170\n",
      "Epoch 103/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3495 - accuracy: 0.8453 - val_loss: 0.8683 - val_accuracy: 0.7165\n",
      "Epoch 104/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3406 - accuracy: 0.8424 - val_loss: 0.8622 - val_accuracy: 0.6994\n",
      "Epoch 105/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3362 - accuracy: 0.8467 - val_loss: 0.9055 - val_accuracy: 0.7133\n",
      "Epoch 106/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3368 - accuracy: 0.8411 - val_loss: 0.7647 - val_accuracy: 0.7101\n",
      "Epoch 107/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3370 - accuracy: 0.8521 - val_loss: 0.8947 - val_accuracy: 0.6763\n",
      "Epoch 108/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3394 - accuracy: 0.8421 - val_loss: 0.8785 - val_accuracy: 0.7042\n",
      "Epoch 109/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3392 - accuracy: 0.8435 - val_loss: 0.9205 - val_accuracy: 0.7079\n",
      "Epoch 110/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3359 - accuracy: 0.8507 - val_loss: 0.6918 - val_accuracy: 0.7122\n",
      "Epoch 111/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3370 - accuracy: 0.8491 - val_loss: 0.8961 - val_accuracy: 0.7122\n",
      "Epoch 112/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3381 - accuracy: 0.8464 - val_loss: 0.8800 - val_accuracy: 0.7031\n",
      "Epoch 113/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3385 - accuracy: 0.8467 - val_loss: 0.9367 - val_accuracy: 0.6919\n",
      "Epoch 114/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3280 - accuracy: 0.8515 - val_loss: 0.7818 - val_accuracy: 0.7138\n",
      "Epoch 115/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3260 - accuracy: 0.8550 - val_loss: 0.9084 - val_accuracy: 0.7165\n",
      "Epoch 116/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3275 - accuracy: 0.8531 - val_loss: 0.9439 - val_accuracy: 0.7031\n",
      "Epoch 117/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3347 - accuracy: 0.8512 - val_loss: 1.0363 - val_accuracy: 0.7186\n",
      "Epoch 118/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3306 - accuracy: 0.8545 - val_loss: 0.8480 - val_accuracy: 0.7288\n",
      "Epoch 119/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3358 - accuracy: 0.8456 - val_loss: 0.9080 - val_accuracy: 0.7095\n",
      "Epoch 120/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3345 - accuracy: 0.8496 - val_loss: 1.0132 - val_accuracy: 0.6994\n",
      "Epoch 121/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3253 - accuracy: 0.8499 - val_loss: 0.9424 - val_accuracy: 0.7235\n",
      "Epoch 122/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3201 - accuracy: 0.8609 - val_loss: 0.9665 - val_accuracy: 0.7165\n",
      "Epoch 123/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3144 - accuracy: 0.8636 - val_loss: 0.8702 - val_accuracy: 0.7133\n",
      "Epoch 124/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3250 - accuracy: 0.8504 - val_loss: 0.8516 - val_accuracy: 0.7224\n",
      "Epoch 125/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3163 - accuracy: 0.8588 - val_loss: 0.8743 - val_accuracy: 0.7251\n",
      "Epoch 126/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3150 - accuracy: 0.8579 - val_loss: 0.9109 - val_accuracy: 0.7138\n",
      "Epoch 127/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3137 - accuracy: 0.8550 - val_loss: 1.0018 - val_accuracy: 0.7213\n",
      "Epoch 128/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3145 - accuracy: 0.8593 - val_loss: 0.8672 - val_accuracy: 0.7181\n",
      "Epoch 129/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3162 - accuracy: 0.8566 - val_loss: 0.9355 - val_accuracy: 0.7181\n",
      "Epoch 130/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3166 - accuracy: 0.8609 - val_loss: 1.7121 - val_accuracy: 0.6554\n",
      "Epoch 131/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3074 - accuracy: 0.8606 - val_loss: 1.0074 - val_accuracy: 0.7235\n",
      "Epoch 132/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3144 - accuracy: 0.8590 - val_loss: 0.9110 - val_accuracy: 0.7208\n",
      "Epoch 133/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3099 - accuracy: 0.8646 - val_loss: 0.8565 - val_accuracy: 0.7004\n",
      "Epoch 134/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3082 - accuracy: 0.8681 - val_loss: 0.9878 - val_accuracy: 0.7063\n",
      "Epoch 135/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3053 - accuracy: 0.8646 - val_loss: 0.9820 - val_accuracy: 0.7042\n",
      "Epoch 136/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3014 - accuracy: 0.8673 - val_loss: 1.0923 - val_accuracy: 0.7176\n",
      "Epoch 137/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3085 - accuracy: 0.8660 - val_loss: 1.0943 - val_accuracy: 0.7010\n",
      "Epoch 138/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2960 - accuracy: 0.8676 - val_loss: 0.9900 - val_accuracy: 0.7095\n",
      "Epoch 139/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2850 - accuracy: 0.8716 - val_loss: 0.8989 - val_accuracy: 0.7197\n",
      "Epoch 140/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3041 - accuracy: 0.8614 - val_loss: 0.8542 - val_accuracy: 0.7208\n",
      "Epoch 141/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2889 - accuracy: 0.8689 - val_loss: 0.9953 - val_accuracy: 0.6994\n",
      "Epoch 142/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3064 - accuracy: 0.8596 - val_loss: 0.9294 - val_accuracy: 0.7245\n",
      "Epoch 143/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3006 - accuracy: 0.8663 - val_loss: 0.9757 - val_accuracy: 0.7235\n",
      "Epoch 144/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3017 - accuracy: 0.8673 - val_loss: 1.0905 - val_accuracy: 0.7256\n",
      "Epoch 145/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2925 - accuracy: 0.8673 - val_loss: 1.0927 - val_accuracy: 0.7272\n",
      "Epoch 146/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2987 - accuracy: 0.8644 - val_loss: 1.0182 - val_accuracy: 0.7042\n",
      "Epoch 147/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2872 - accuracy: 0.8676 - val_loss: 0.9792 - val_accuracy: 0.7203\n",
      "Epoch 148/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2923 - accuracy: 0.8687 - val_loss: 0.9414 - val_accuracy: 0.7235\n",
      "Epoch 149/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2872 - accuracy: 0.8727 - val_loss: 1.0183 - val_accuracy: 0.7197\n",
      "Epoch 150/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.3074 - accuracy: 0.8673 - val_loss: 0.9681 - val_accuracy: 0.7117\n",
      "Epoch 151/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2884 - accuracy: 0.8708 - val_loss: 0.9293 - val_accuracy: 0.7251\n",
      "Epoch 152/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2907 - accuracy: 0.8711 - val_loss: 0.9894 - val_accuracy: 0.7138\n",
      "Epoch 153/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2835 - accuracy: 0.8740 - val_loss: 1.2168 - val_accuracy: 0.7165\n",
      "Epoch 154/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2882 - accuracy: 0.8732 - val_loss: 0.9857 - val_accuracy: 0.7122\n",
      "Epoch 155/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2935 - accuracy: 0.8722 - val_loss: 1.0054 - val_accuracy: 0.7090\n",
      "Epoch 156/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2896 - accuracy: 0.8687 - val_loss: 1.1866 - val_accuracy: 0.7058\n",
      "Epoch 157/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2912 - accuracy: 0.8754 - val_loss: 1.2155 - val_accuracy: 0.7138\n",
      "Epoch 158/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2835 - accuracy: 0.8770 - val_loss: 0.9223 - val_accuracy: 0.7128\n",
      "Epoch 159/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2850 - accuracy: 0.8775 - val_loss: 1.1648 - val_accuracy: 0.7197\n",
      "Epoch 160/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2860 - accuracy: 0.8751 - val_loss: 1.1077 - val_accuracy: 0.7278\n",
      "Epoch 161/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2814 - accuracy: 0.8719 - val_loss: 1.0113 - val_accuracy: 0.7122\n",
      "Epoch 162/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2920 - accuracy: 0.8668 - val_loss: 1.0110 - val_accuracy: 0.7235\n",
      "Epoch 163/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2857 - accuracy: 0.8708 - val_loss: 1.1732 - val_accuracy: 0.7176\n",
      "Epoch 164/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2760 - accuracy: 0.8845 - val_loss: 0.9928 - val_accuracy: 0.7154\n",
      "Epoch 165/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2750 - accuracy: 0.8767 - val_loss: 1.1246 - val_accuracy: 0.7251\n",
      "Epoch 166/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2779 - accuracy: 0.8789 - val_loss: 1.2518 - val_accuracy: 0.6854\n",
      "Epoch 167/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2688 - accuracy: 0.8772 - val_loss: 1.1608 - val_accuracy: 0.7117\n",
      "Epoch 168/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2733 - accuracy: 0.8789 - val_loss: 1.2115 - val_accuracy: 0.7133\n",
      "Epoch 169/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2812 - accuracy: 0.8821 - val_loss: 1.1383 - val_accuracy: 0.7133\n",
      "Epoch 170/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2667 - accuracy: 0.8815 - val_loss: 1.3000 - val_accuracy: 0.7020\n",
      "Epoch 171/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2650 - accuracy: 0.8802 - val_loss: 1.1852 - val_accuracy: 0.7128\n",
      "Epoch 172/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2764 - accuracy: 0.8716 - val_loss: 1.2254 - val_accuracy: 0.7026\n",
      "Epoch 173/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2815 - accuracy: 0.8719 - val_loss: 1.2732 - val_accuracy: 0.7240\n",
      "Epoch 174/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2716 - accuracy: 0.8772 - val_loss: 0.9291 - val_accuracy: 0.7042\n",
      "Epoch 175/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2735 - accuracy: 0.8767 - val_loss: 1.2338 - val_accuracy: 0.7256\n",
      "Epoch 176/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2658 - accuracy: 0.8823 - val_loss: 1.3190 - val_accuracy: 0.7095\n",
      "Epoch 177/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2535 - accuracy: 0.8850 - val_loss: 1.1043 - val_accuracy: 0.7181\n",
      "Epoch 178/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2669 - accuracy: 0.8775 - val_loss: 1.0922 - val_accuracy: 0.7036\n",
      "Epoch 179/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2685 - accuracy: 0.8839 - val_loss: 1.1681 - val_accuracy: 0.7256\n",
      "Epoch 180/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2718 - accuracy: 0.8821 - val_loss: 0.9548 - val_accuracy: 0.7176\n",
      "Epoch 181/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2681 - accuracy: 0.8837 - val_loss: 0.9846 - val_accuracy: 0.7160\n",
      "Epoch 182/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2705 - accuracy: 0.8823 - val_loss: 1.0705 - val_accuracy: 0.7272\n",
      "Epoch 183/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2679 - accuracy: 0.8789 - val_loss: 1.1081 - val_accuracy: 0.7122\n",
      "Epoch 184/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2616 - accuracy: 0.8807 - val_loss: 1.1424 - val_accuracy: 0.7117\n",
      "Epoch 185/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2585 - accuracy: 0.8850 - val_loss: 1.2925 - val_accuracy: 0.7219\n",
      "Epoch 186/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2733 - accuracy: 0.8791 - val_loss: 1.1693 - val_accuracy: 0.7170\n",
      "Epoch 187/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2458 - accuracy: 0.8837 - val_loss: 1.2997 - val_accuracy: 0.7176\n",
      "Epoch 188/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2578 - accuracy: 0.8813 - val_loss: 1.0827 - val_accuracy: 0.7176\n",
      "Epoch 189/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2672 - accuracy: 0.8823 - val_loss: 1.1092 - val_accuracy: 0.7229\n",
      "Epoch 190/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2688 - accuracy: 0.8864 - val_loss: 0.9745 - val_accuracy: 0.7133\n",
      "Epoch 191/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2518 - accuracy: 0.8856 - val_loss: 1.0779 - val_accuracy: 0.7208\n",
      "Epoch 192/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2612 - accuracy: 0.8896 - val_loss: 1.4860 - val_accuracy: 0.7122\n",
      "Epoch 193/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2508 - accuracy: 0.8901 - val_loss: 1.0307 - val_accuracy: 0.7106\n",
      "Epoch 194/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2429 - accuracy: 0.8877 - val_loss: 0.9075 - val_accuracy: 0.7181\n",
      "Epoch 195/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2550 - accuracy: 0.8818 - val_loss: 1.2551 - val_accuracy: 0.7085\n",
      "Epoch 196/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2661 - accuracy: 0.8810 - val_loss: 1.1899 - val_accuracy: 0.7224\n",
      "Epoch 197/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2526 - accuracy: 0.8850 - val_loss: 1.2115 - val_accuracy: 0.7197\n",
      "Epoch 198/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2559 - accuracy: 0.8888 - val_loss: 1.0204 - val_accuracy: 0.7133\n",
      "Epoch 199/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2472 - accuracy: 0.8885 - val_loss: 1.2399 - val_accuracy: 0.7186\n",
      "Epoch 200/200\n",
      "3731/3731 [==============================] - 26s 7ms/step - loss: 0.2532 - accuracy: 0.8872 - val_loss: 0.9508 - val_accuracy: 0.7192\n",
      "accuracy: 73.42%\n",
      "Confusion Matrix - Test\n",
      "[[540   0 259]\n",
      " [  5   8  27]\n",
      " [196   9 822]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Apr-May       0.73      0.68      0.70       799\n",
      "     Jan-Feb       0.47      0.20      0.28        40\n",
      "         Mar       0.74      0.80      0.77      1027\n",
      "\n",
      "    accuracy                           0.73      1866\n",
      "   macro avg       0.65      0.56      0.58      1866\n",
      "weighted avg       0.73      0.73      0.73      1866\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Train on 3732 samples, validate on 1865 samples\n",
      "Epoch 1/200\n",
      "3732/3732 [==============================] - 28s 8ms/step - loss: 1.0493 - accuracy: 0.5839 - val_loss: 0.8481 - val_accuracy: 0.4279\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 2/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.7868 - accuracy: 0.6083 - val_loss: 0.8165 - val_accuracy: 0.5512\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 3/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.7275 - accuracy: 0.6249 - val_loss: 1.0043 - val_accuracy: 0.5555\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 4/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.6966 - accuracy: 0.6399 - val_loss: 1.0763 - val_accuracy: 0.5684\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 5/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.6574 - accuracy: 0.6632 - val_loss: 0.7718 - val_accuracy: 0.5871\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 6/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.6314 - accuracy: 0.6744 - val_loss: 0.6658 - val_accuracy: 0.6751\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 7/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.6151 - accuracy: 0.6916 - val_loss: 0.6836 - val_accuracy: 0.6810\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 8/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5960 - accuracy: 0.6977 - val_loss: 0.6358 - val_accuracy: 0.6879\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 9/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.6007 - accuracy: 0.7031 - val_loss: 0.6242 - val_accuracy: 0.6820\n",
      "Epoch 10/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5876 - accuracy: 0.7026 - val_loss: 0.7019 - val_accuracy: 0.6869\n",
      "Epoch 11/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5713 - accuracy: 0.7221 - val_loss: 0.6941 - val_accuracy: 0.6735\n",
      "Epoch 12/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5629 - accuracy: 0.7181 - val_loss: 0.6438 - val_accuracy: 0.6901\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 13/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5601 - accuracy: 0.7195 - val_loss: 0.7249 - val_accuracy: 0.6466\n",
      "Epoch 14/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5467 - accuracy: 0.7339 - val_loss: 0.6279 - val_accuracy: 0.6987\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 15/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5521 - accuracy: 0.7221 - val_loss: 0.6672 - val_accuracy: 0.6917\n",
      "Epoch 16/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5414 - accuracy: 0.7342 - val_loss: 0.6687 - val_accuracy: 0.6858\n",
      "Epoch 17/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5472 - accuracy: 0.7350 - val_loss: 0.6799 - val_accuracy: 0.6853\n",
      "Epoch 18/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5287 - accuracy: 0.7438 - val_loss: 0.7634 - val_accuracy: 0.6123\n",
      "Epoch 19/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5275 - accuracy: 0.7422 - val_loss: 0.7061 - val_accuracy: 0.6638\n",
      "Epoch 20/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5280 - accuracy: 0.7497 - val_loss: 0.6109 - val_accuracy: 0.7029\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 21/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5230 - accuracy: 0.7441 - val_loss: 0.6695 - val_accuracy: 0.6949\n",
      "Epoch 22/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5206 - accuracy: 0.7497 - val_loss: 0.8076 - val_accuracy: 0.6284\n",
      "Epoch 23/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5129 - accuracy: 0.7500 - val_loss: 0.6854 - val_accuracy: 0.7008\n",
      "Epoch 24/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5012 - accuracy: 0.7580 - val_loss: 0.6519 - val_accuracy: 0.7051\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 25/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.5122 - accuracy: 0.7575 - val_loss: 0.6722 - val_accuracy: 0.6879\n",
      "Epoch 26/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4947 - accuracy: 0.7637 - val_loss: 0.6472 - val_accuracy: 0.7062\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 27/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4921 - accuracy: 0.7623 - val_loss: 0.7269 - val_accuracy: 0.7008\n",
      "Epoch 28/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4875 - accuracy: 0.7615 - val_loss: 0.6713 - val_accuracy: 0.7024\n",
      "Epoch 29/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4910 - accuracy: 0.7564 - val_loss: 0.6368 - val_accuracy: 0.7201\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 30/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4764 - accuracy: 0.7680 - val_loss: 0.6722 - val_accuracy: 0.7078\n",
      "Epoch 31/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4829 - accuracy: 0.7698 - val_loss: 0.6792 - val_accuracy: 0.7035\n",
      "Epoch 32/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4697 - accuracy: 0.7738 - val_loss: 0.7097 - val_accuracy: 0.7040\n",
      "Epoch 33/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4655 - accuracy: 0.7795 - val_loss: 0.6947 - val_accuracy: 0.6853\n",
      "Epoch 34/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4677 - accuracy: 0.7752 - val_loss: 0.7677 - val_accuracy: 0.6799\n",
      "Epoch 35/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4647 - accuracy: 0.7830 - val_loss: 0.6550 - val_accuracy: 0.7046\n",
      "Epoch 36/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4539 - accuracy: 0.7827 - val_loss: 0.7111 - val_accuracy: 0.7046\n",
      "Epoch 37/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4551 - accuracy: 0.7827 - val_loss: 0.6694 - val_accuracy: 0.7212\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 38/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4520 - accuracy: 0.7921 - val_loss: 1.0127 - val_accuracy: 0.6354\n",
      "Epoch 39/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4439 - accuracy: 0.7918 - val_loss: 0.6736 - val_accuracy: 0.7147\n",
      "Epoch 40/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4462 - accuracy: 0.7921 - val_loss: 0.7475 - val_accuracy: 0.7019\n",
      "Epoch 41/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4452 - accuracy: 0.7835 - val_loss: 0.7363 - val_accuracy: 0.6971\n",
      "Epoch 42/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4385 - accuracy: 0.7913 - val_loss: 0.7298 - val_accuracy: 0.6992\n",
      "Epoch 43/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4320 - accuracy: 0.7985 - val_loss: 0.6880 - val_accuracy: 0.7110\n",
      "Epoch 44/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4380 - accuracy: 0.7921 - val_loss: 0.6865 - val_accuracy: 0.7040\n",
      "Epoch 45/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4382 - accuracy: 0.7990 - val_loss: 0.8121 - val_accuracy: 0.6702\n",
      "Epoch 46/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4293 - accuracy: 0.8068 - val_loss: 0.6617 - val_accuracy: 0.7164\n",
      "Epoch 47/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4223 - accuracy: 0.8047 - val_loss: 0.7476 - val_accuracy: 0.7131\n",
      "Epoch 48/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4221 - accuracy: 0.7964 - val_loss: 0.7565 - val_accuracy: 0.7035\n",
      "Epoch 49/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4266 - accuracy: 0.8014 - val_loss: 0.7929 - val_accuracy: 0.7003\n",
      "Epoch 50/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4330 - accuracy: 0.7923 - val_loss: 0.9492 - val_accuracy: 0.6708\n",
      "Epoch 51/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4209 - accuracy: 0.8079 - val_loss: 0.7298 - val_accuracy: 0.7190\n",
      "Epoch 52/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4051 - accuracy: 0.8063 - val_loss: 0.7662 - val_accuracy: 0.7088\n",
      "Epoch 53/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4044 - accuracy: 0.8114 - val_loss: 0.7006 - val_accuracy: 0.7201\n",
      "Epoch 54/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4227 - accuracy: 0.7977 - val_loss: 0.7064 - val_accuracy: 0.7190\n",
      "Epoch 55/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4142 - accuracy: 0.8108 - val_loss: 0.7443 - val_accuracy: 0.7110\n",
      "Epoch 56/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4195 - accuracy: 0.8009 - val_loss: 0.7655 - val_accuracy: 0.7008\n",
      "Epoch 57/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4052 - accuracy: 0.8087 - val_loss: 0.8196 - val_accuracy: 0.6976\n",
      "Epoch 58/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4096 - accuracy: 0.8089 - val_loss: 0.7218 - val_accuracy: 0.7255\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 59/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4065 - accuracy: 0.8098 - val_loss: 0.8052 - val_accuracy: 0.7056\n",
      "Epoch 60/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4035 - accuracy: 0.8140 - val_loss: 0.8609 - val_accuracy: 0.6863\n",
      "Epoch 61/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4027 - accuracy: 0.8154 - val_loss: 1.0006 - val_accuracy: 0.6681\n",
      "Epoch 62/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3971 - accuracy: 0.8122 - val_loss: 0.7712 - val_accuracy: 0.7153\n",
      "Epoch 63/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3996 - accuracy: 0.8119 - val_loss: 0.8649 - val_accuracy: 0.6799\n",
      "Epoch 64/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4014 - accuracy: 0.8170 - val_loss: 0.7802 - val_accuracy: 0.7051\n",
      "Epoch 65/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3921 - accuracy: 0.8232 - val_loss: 1.2605 - val_accuracy: 0.6172\n",
      "Epoch 66/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.4000 - accuracy: 0.8065 - val_loss: 0.7901 - val_accuracy: 0.6826\n",
      "Epoch 67/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3948 - accuracy: 0.8186 - val_loss: 0.7542 - val_accuracy: 0.7196\n",
      "Epoch 68/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3902 - accuracy: 0.8240 - val_loss: 0.9248 - val_accuracy: 0.6826\n",
      "Epoch 69/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3837 - accuracy: 0.8269 - val_loss: 0.8149 - val_accuracy: 0.6981\n",
      "Epoch 70/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3869 - accuracy: 0.8261 - val_loss: 0.7857 - val_accuracy: 0.7099\n",
      "Epoch 71/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3844 - accuracy: 0.8274 - val_loss: 0.9170 - val_accuracy: 0.6847\n",
      "Epoch 72/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3726 - accuracy: 0.8280 - val_loss: 0.7488 - val_accuracy: 0.7051\n",
      "Epoch 73/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3709 - accuracy: 0.8293 - val_loss: 0.7651 - val_accuracy: 0.7174\n",
      "Epoch 74/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3780 - accuracy: 0.8256 - val_loss: 0.7753 - val_accuracy: 0.7046\n",
      "Epoch 75/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3798 - accuracy: 0.8253 - val_loss: 0.8529 - val_accuracy: 0.7115\n",
      "Epoch 76/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3670 - accuracy: 0.8307 - val_loss: 0.7227 - val_accuracy: 0.7206\n",
      "Epoch 77/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3737 - accuracy: 0.8309 - val_loss: 1.1179 - val_accuracy: 0.6756\n",
      "Epoch 78/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3784 - accuracy: 0.8298 - val_loss: 0.9012 - val_accuracy: 0.6858\n",
      "Epoch 79/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3664 - accuracy: 0.8349 - val_loss: 0.8335 - val_accuracy: 0.7287\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 80/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3690 - accuracy: 0.8304 - val_loss: 0.8005 - val_accuracy: 0.7137\n",
      "Epoch 81/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3695 - accuracy: 0.8290 - val_loss: 0.7965 - val_accuracy: 0.7105\n",
      "Epoch 82/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3694 - accuracy: 0.8341 - val_loss: 0.7909 - val_accuracy: 0.7180\n",
      "Epoch 83/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3586 - accuracy: 0.8288 - val_loss: 0.8605 - val_accuracy: 0.7008\n",
      "Epoch 84/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3623 - accuracy: 0.8363 - val_loss: 0.9263 - val_accuracy: 0.6901\n",
      "Epoch 85/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3624 - accuracy: 0.8272 - val_loss: 0.7727 - val_accuracy: 0.7190\n",
      "Epoch 86/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3598 - accuracy: 0.8307 - val_loss: 0.8637 - val_accuracy: 0.7180\n",
      "Epoch 87/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3711 - accuracy: 0.8336 - val_loss: 0.9133 - val_accuracy: 0.6981\n",
      "Epoch 88/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3529 - accuracy: 0.8365 - val_loss: 0.8072 - val_accuracy: 0.7024\n",
      "Epoch 89/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3646 - accuracy: 0.8301 - val_loss: 0.9547 - val_accuracy: 0.7174\n",
      "Epoch 90/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3568 - accuracy: 0.8382 - val_loss: 0.7762 - val_accuracy: 0.7169\n",
      "Epoch 91/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3598 - accuracy: 0.8368 - val_loss: 0.8110 - val_accuracy: 0.7126\n",
      "Epoch 92/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3483 - accuracy: 0.8427 - val_loss: 0.8778 - val_accuracy: 0.7147\n",
      "Epoch 93/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3654 - accuracy: 0.8323 - val_loss: 1.1646 - val_accuracy: 0.6788\n",
      "Epoch 94/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3502 - accuracy: 0.8376 - val_loss: 0.8543 - val_accuracy: 0.6965\n",
      "Epoch 95/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3441 - accuracy: 0.8408 - val_loss: 0.7879 - val_accuracy: 0.7153\n",
      "Epoch 96/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3489 - accuracy: 0.8435 - val_loss: 0.7987 - val_accuracy: 0.7126\n",
      "Epoch 97/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3575 - accuracy: 0.8360 - val_loss: 0.8073 - val_accuracy: 0.7078\n",
      "Epoch 98/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3497 - accuracy: 0.8432 - val_loss: 0.9225 - val_accuracy: 0.6735\n",
      "Epoch 99/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3459 - accuracy: 0.8347 - val_loss: 0.8684 - val_accuracy: 0.7051\n",
      "Epoch 100/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3516 - accuracy: 0.8392 - val_loss: 1.1576 - val_accuracy: 0.6692\n",
      "Epoch 101/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3426 - accuracy: 0.8478 - val_loss: 0.9321 - val_accuracy: 0.7088\n",
      "Epoch 102/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3441 - accuracy: 0.8454 - val_loss: 0.8183 - val_accuracy: 0.7056\n",
      "Epoch 103/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3400 - accuracy: 0.8465 - val_loss: 0.8866 - val_accuracy: 0.7029\n",
      "Epoch 104/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3341 - accuracy: 0.8462 - val_loss: 0.9887 - val_accuracy: 0.7072\n",
      "Epoch 105/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3393 - accuracy: 0.8508 - val_loss: 0.8139 - val_accuracy: 0.7040\n",
      "Epoch 106/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3343 - accuracy: 0.8483 - val_loss: 1.0257 - val_accuracy: 0.6547\n",
      "Epoch 107/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3397 - accuracy: 0.8470 - val_loss: 1.0458 - val_accuracy: 0.6992\n",
      "Epoch 108/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3336 - accuracy: 0.8408 - val_loss: 0.7877 - val_accuracy: 0.7115\n",
      "Epoch 109/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3349 - accuracy: 0.8470 - val_loss: 0.8567 - val_accuracy: 0.7131\n",
      "Epoch 110/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3380 - accuracy: 0.8473 - val_loss: 0.9486 - val_accuracy: 0.7035\n",
      "Epoch 111/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3348 - accuracy: 0.8499 - val_loss: 1.0014 - val_accuracy: 0.7035\n",
      "Epoch 112/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3269 - accuracy: 0.8483 - val_loss: 0.8878 - val_accuracy: 0.7110\n",
      "Epoch 113/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3297 - accuracy: 0.8491 - val_loss: 0.8859 - val_accuracy: 0.7126\n",
      "Epoch 114/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3290 - accuracy: 0.8545 - val_loss: 1.2933 - val_accuracy: 0.6735\n",
      "Epoch 115/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3273 - accuracy: 0.8483 - val_loss: 0.9060 - val_accuracy: 0.7292\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 116/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3329 - accuracy: 0.8569 - val_loss: 0.8308 - val_accuracy: 0.7056\n",
      "Epoch 117/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3341 - accuracy: 0.8454 - val_loss: 0.9697 - val_accuracy: 0.7105\n",
      "Epoch 118/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3226 - accuracy: 0.8604 - val_loss: 1.0172 - val_accuracy: 0.7223\n",
      "Epoch 119/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3206 - accuracy: 0.8561 - val_loss: 0.9629 - val_accuracy: 0.7233\n",
      "Epoch 120/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3210 - accuracy: 0.8524 - val_loss: 2.5369 - val_accuracy: 0.6263\n",
      "Epoch 121/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3191 - accuracy: 0.8553 - val_loss: 0.9875 - val_accuracy: 0.7282\n",
      "Epoch 122/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3166 - accuracy: 0.8599 - val_loss: 0.8989 - val_accuracy: 0.7164\n",
      "Epoch 123/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3086 - accuracy: 0.8644 - val_loss: 0.8490 - val_accuracy: 0.7110\n",
      "Epoch 124/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3139 - accuracy: 0.8607 - val_loss: 0.9104 - val_accuracy: 0.7185\n",
      "Epoch 125/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3157 - accuracy: 0.8591 - val_loss: 0.8553 - val_accuracy: 0.7314\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 126/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3205 - accuracy: 0.8588 - val_loss: 0.8690 - val_accuracy: 0.7115\n",
      "Epoch 127/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3158 - accuracy: 0.8625 - val_loss: 0.8023 - val_accuracy: 0.7276\n",
      "Epoch 128/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3033 - accuracy: 0.8652 - val_loss: 0.8652 - val_accuracy: 0.7255\n",
      "Epoch 129/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3071 - accuracy: 0.8650 - val_loss: 0.9872 - val_accuracy: 0.7217\n",
      "Epoch 130/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3077 - accuracy: 0.8676 - val_loss: 0.9659 - val_accuracy: 0.7121\n",
      "Epoch 131/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3038 - accuracy: 0.8679 - val_loss: 1.3643 - val_accuracy: 0.7035\n",
      "Epoch 132/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2978 - accuracy: 0.8682 - val_loss: 0.9741 - val_accuracy: 0.7019\n",
      "Epoch 133/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.3125 - accuracy: 0.8674 - val_loss: 1.0350 - val_accuracy: 0.7249\n",
      "Epoch 134/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2971 - accuracy: 0.8700 - val_loss: 0.8826 - val_accuracy: 0.7131\n",
      "Epoch 135/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2989 - accuracy: 0.8722 - val_loss: 1.0372 - val_accuracy: 0.7292\n",
      "Epoch 136/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2893 - accuracy: 0.8708 - val_loss: 0.8979 - val_accuracy: 0.7217\n",
      "Epoch 137/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2865 - accuracy: 0.8800 - val_loss: 0.8852 - val_accuracy: 0.7217\n",
      "Epoch 138/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2937 - accuracy: 0.8690 - val_loss: 1.6441 - val_accuracy: 0.6617\n",
      "Epoch 139/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2959 - accuracy: 0.8730 - val_loss: 1.0204 - val_accuracy: 0.7276\n",
      "Epoch 140/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2955 - accuracy: 0.8700 - val_loss: 0.9949 - val_accuracy: 0.7196\n",
      "Epoch 141/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2900 - accuracy: 0.8703 - val_loss: 0.8456 - val_accuracy: 0.7249\n",
      "Epoch 142/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2857 - accuracy: 0.8765 - val_loss: 0.9183 - val_accuracy: 0.7340\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 143/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2887 - accuracy: 0.8754 - val_loss: 1.0431 - val_accuracy: 0.7228\n",
      "Epoch 144/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2963 - accuracy: 0.8722 - val_loss: 0.9902 - val_accuracy: 0.7276\n",
      "Epoch 145/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2898 - accuracy: 0.8727 - val_loss: 1.2304 - val_accuracy: 0.7040\n",
      "Epoch 146/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2779 - accuracy: 0.8781 - val_loss: 0.9803 - val_accuracy: 0.7206\n",
      "Epoch 147/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2879 - accuracy: 0.8754 - val_loss: 0.9704 - val_accuracy: 0.7265\n",
      "Epoch 148/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2911 - accuracy: 0.8703 - val_loss: 0.8776 - val_accuracy: 0.7265\n",
      "Epoch 149/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2912 - accuracy: 0.8751 - val_loss: 1.1635 - val_accuracy: 0.6928\n",
      "Epoch 150/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2771 - accuracy: 0.8794 - val_loss: 0.9504 - val_accuracy: 0.7298\n",
      "Epoch 151/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2880 - accuracy: 0.8751 - val_loss: 1.0594 - val_accuracy: 0.7324\n",
      "Epoch 152/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2882 - accuracy: 0.8719 - val_loss: 0.9320 - val_accuracy: 0.7340\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 153/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2850 - accuracy: 0.8789 - val_loss: 2.4361 - val_accuracy: 0.6450\n",
      "Epoch 154/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2805 - accuracy: 0.8773 - val_loss: 1.0403 - val_accuracy: 0.7287\n",
      "Epoch 155/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2841 - accuracy: 0.8767 - val_loss: 1.1211 - val_accuracy: 0.7217\n",
      "Epoch 156/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2821 - accuracy: 0.8735 - val_loss: 0.8862 - val_accuracy: 0.7147\n",
      "Epoch 157/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2683 - accuracy: 0.8832 - val_loss: 1.1500 - val_accuracy: 0.7303\n",
      "Epoch 158/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2802 - accuracy: 0.8800 - val_loss: 1.5998 - val_accuracy: 0.7217\n",
      "Epoch 159/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2776 - accuracy: 0.8743 - val_loss: 0.8143 - val_accuracy: 0.7158\n",
      "Epoch 160/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2744 - accuracy: 0.8789 - val_loss: 0.8399 - val_accuracy: 0.7292\n",
      "Epoch 161/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2807 - accuracy: 0.8802 - val_loss: 1.2599 - val_accuracy: 0.7416\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 162/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2807 - accuracy: 0.8773 - val_loss: 0.9604 - val_accuracy: 0.7233\n",
      "Epoch 163/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2705 - accuracy: 0.8832 - val_loss: 1.1667 - val_accuracy: 0.7383\n",
      "Epoch 164/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2706 - accuracy: 0.8821 - val_loss: 1.1827 - val_accuracy: 0.7244\n",
      "Epoch 165/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2785 - accuracy: 0.8754 - val_loss: 0.9020 - val_accuracy: 0.7335\n",
      "Epoch 166/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2736 - accuracy: 0.8810 - val_loss: 4.2351 - val_accuracy: 0.5995\n",
      "Epoch 167/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2646 - accuracy: 0.8877 - val_loss: 1.0594 - val_accuracy: 0.7217\n",
      "Epoch 168/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2572 - accuracy: 0.8853 - val_loss: 1.2214 - val_accuracy: 0.7271\n",
      "Epoch 169/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2847 - accuracy: 0.8759 - val_loss: 1.1990 - val_accuracy: 0.7201\n",
      "Epoch 170/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2633 - accuracy: 0.8856 - val_loss: 1.1188 - val_accuracy: 0.7357\n",
      "Epoch 171/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2618 - accuracy: 0.8917 - val_loss: 1.3240 - val_accuracy: 0.7351\n",
      "Epoch 172/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2618 - accuracy: 0.8792 - val_loss: 1.0675 - val_accuracy: 0.7357\n",
      "Epoch 173/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2769 - accuracy: 0.8770 - val_loss: 0.8008 - val_accuracy: 0.7367\n",
      "Epoch 174/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2586 - accuracy: 0.8883 - val_loss: 0.8868 - val_accuracy: 0.7260\n",
      "Epoch 175/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2738 - accuracy: 0.8821 - val_loss: 1.0304 - val_accuracy: 0.7335\n",
      "Epoch 176/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2669 - accuracy: 0.8783 - val_loss: 1.2460 - val_accuracy: 0.7346\n",
      "Epoch 177/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2553 - accuracy: 0.8917 - val_loss: 1.0010 - val_accuracy: 0.7137\n",
      "Epoch 178/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2560 - accuracy: 0.8834 - val_loss: 0.9795 - val_accuracy: 0.7201\n",
      "Epoch 179/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2681 - accuracy: 0.8826 - val_loss: 1.4187 - val_accuracy: 0.7265\n",
      "Epoch 180/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2533 - accuracy: 0.8928 - val_loss: 1.0915 - val_accuracy: 0.7185\n",
      "Epoch 181/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2550 - accuracy: 0.8934 - val_loss: 1.4248 - val_accuracy: 0.7340\n",
      "Epoch 182/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2565 - accuracy: 0.8907 - val_loss: 1.1643 - val_accuracy: 0.7314\n",
      "Epoch 183/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2583 - accuracy: 0.8867 - val_loss: 0.9129 - val_accuracy: 0.7298\n",
      "Epoch 184/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2673 - accuracy: 0.8810 - val_loss: 1.2428 - val_accuracy: 0.7469\n",
      "Validation Accuracy has improved. Saving Model.\n",
      "Epoch 185/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2611 - accuracy: 0.8944 - val_loss: 0.9643 - val_accuracy: 0.7233\n",
      "Epoch 186/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2465 - accuracy: 0.8966 - val_loss: 0.9959 - val_accuracy: 0.7271\n",
      "Epoch 187/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2631 - accuracy: 0.8896 - val_loss: 1.5476 - val_accuracy: 0.7389\n",
      "Epoch 188/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2415 - accuracy: 0.8926 - val_loss: 0.8746 - val_accuracy: 0.7158\n",
      "Epoch 189/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2481 - accuracy: 0.8917 - val_loss: 1.2701 - val_accuracy: 0.7265\n",
      "Epoch 190/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2472 - accuracy: 0.8926 - val_loss: 1.0693 - val_accuracy: 0.7174\n",
      "Epoch 191/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2431 - accuracy: 0.8939 - val_loss: 1.1573 - val_accuracy: 0.7362\n",
      "Epoch 192/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2674 - accuracy: 0.8848 - val_loss: 1.4654 - val_accuracy: 0.7244\n",
      "Epoch 193/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2490 - accuracy: 0.8939 - val_loss: 1.3087 - val_accuracy: 0.7362\n",
      "Epoch 194/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2481 - accuracy: 0.8917 - val_loss: 1.0747 - val_accuracy: 0.7287\n",
      "Epoch 195/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2515 - accuracy: 0.8912 - val_loss: 1.4402 - val_accuracy: 0.7405\n",
      "Epoch 196/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2565 - accuracy: 0.8867 - val_loss: 1.1177 - val_accuracy: 0.7367\n",
      "Epoch 197/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2463 - accuracy: 0.8899 - val_loss: 1.2190 - val_accuracy: 0.7351\n",
      "Epoch 198/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2475 - accuracy: 0.8939 - val_loss: 3.1017 - val_accuracy: 0.7003\n",
      "Epoch 199/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2435 - accuracy: 0.8904 - val_loss: 1.2949 - val_accuracy: 0.7432\n",
      "Epoch 200/200\n",
      "3732/3732 [==============================] - 26s 7ms/step - loss: 0.2388 - accuracy: 0.8992 - val_loss: 1.1295 - val_accuracy: 0.7330\n",
      "accuracy: 74.69%\n",
      "Confusion Matrix - Test\n",
      "[[538   1 259]\n",
      " [  1   9  29]\n",
      " [176   6 846]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Apr-May       0.75      0.67      0.71       798\n",
      "     Jan-Feb       0.56      0.23      0.33        39\n",
      "         Mar       0.75      0.82      0.78      1028\n",
      "\n",
      "    accuracy                           0.75      1865\n",
      "   macro avg       0.69      0.58      0.61      1865\n",
      "weighted avg       0.74      0.75      0.74      1865\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=100)\n",
    "cvscores = []\n",
    "k = 0\n",
    "for train, test in kfold.split(X, Y):\n",
    "    k += 1\n",
    "    batch_size = 32 \n",
    "    num_classes = 3\n",
    "    epochs = 29\n",
    "    img_rows, img_cols = X[train].shape[1], X[train].shape[2]\n",
    "    # X_train = X[train].reshape(X[train].shape[0],img_rows,img_cols,1)\n",
    "    # X_test = X[test].reshape(X[test].shape[0],img_rows,img_cols,1)\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = keras.utils.to_categorical(Y[train], num_classes)\n",
    "    Y_test = keras.utils.to_categorical(Y[test], num_classes)\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(img_rows, img_cols, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "  \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "  \n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.25))\n",
    "  \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history1 = model.fit(X[train], Y_train, batch_size=batch_size, epochs=200, verbose=1, validation_data=(X[test], Y_test), callbacks=[keras_callbacks])\n",
    " \n",
    "    # Evaluate the model\n",
    "    model_sel = load_model('modelCNN.h5')\n",
    "    scores = model_sel.evaluate(X[test], Y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model_sel.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "    # Confusion matrix\n",
    "    Y_pred = model_sel.predict(X[test])\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "    print('Confusion Matrix - Test')\n",
    "    y_val2 = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    print(confusion_matrix(y_val2, y_pred))\n",
    "    print('Classification Report')\n",
    "    target_names = ['Apr-May', 'Jan-Feb', 'Mar']\n",
    "    print(classification_report(y_val2, y_pred, target_names=target_names))\n",
    "    print(\"---------------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "o5T66mzC0WD_",
    "outputId": "3d4df5b6-45b3-4da6-d435-28970e391a56"
   },
   "outputs": [],
   "source": [
    "# for i in range(0, len(cvscores)):\n",
    "#     print(f'> Fold {i+1}: Validation Accuracy = {cvscores[i]}%')\n",
    "\n",
    "# print(f'\\nAverage Validation Accuracy: {np.mean(cvscores)}')\n",
    "# print(f'Standard Deviation: {np.std(cvscores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNNMonths7430.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
